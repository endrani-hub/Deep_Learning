{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need this?\n",
    "Because we need to reduce the loss only, through the change of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear function, loss function can be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Loss = \\frac{1}{m} * \\sum{\\left(x-h\\right)^2}\n",
    "\\end{equation}\n",
    "\n",
    "For sigmoid function, the loss is defined by the entropy as:\n",
    "\n",
    "\\begin{equation}\n",
    "Loss = -\\frac{1}{m} * \\sum{\\left[y\\ln{(h)} + (1-y)\\ln{(1-h)}\\right]}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are the weights(w) and biases(b) and we need to see the change of Loss w.r.t w and b.\n",
    "\n",
    "The way to minimize the Loss function is to find the differentiation w.r.t w and b, and thereabout change the function as shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "w = w - \\alpha * dw\\\\\n",
    "b = b - \\alpha * db\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as **Backpropagation** which tracks back by calculting the ${dw}$ via a step by step approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dw$ and $db$ is derived via:\n",
    "\n",
    "<img src='backprop.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets understand this via a 2 layer architecture.\n",
    "\n",
    "But, before understanding through backpropagation, we are going to understand via Hot and Cold Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>What is **Hot and Cold Learning**?\n",
    "\n",
    "Well its like twisting the parameters a bit higher or a bit lower!\n",
    "Its this which led to Backpropagation.\n",
    "\n",
    "So, lets do this and then move on to Backpropagation.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the packages\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the necessary function\n",
    "\n",
    "def sigmoid(x):\n",
    "    out = 1/(1 + np.exp(-x))\n",
    "    return out\n",
    "\n",
    "def weighted_sum(inp , wt):\n",
    "    assert(len(inp) == len(wt))\n",
    "    out = np.dot(wt.T , inp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the 2 layered nn\n",
    "\n",
    "def nn(inp , weight , bias):\n",
    "    assert(len(inp) == len(weight[0]))\n",
    "    out = np.zeros((len(bias) , 1))\n",
    "    for i in range(len(out)):\n",
    "        out[i] = sigmoid(weighted_sum(inp , weight[i]) + bias[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9168273],\n",
       "       [0.9945137]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = np.array([[0.1,0.2,0.3],[0.4,0.5,0.6]])\n",
    "bias = np.array([1,2])\n",
    "inp = np.array([1,2,3])\n",
    "\n",
    "nn(inp , weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deriving the Hot and Cold training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_out = [0.95 , 1.23]\n",
    "step = 0.005\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(y , h):\n",
    "    cost = np.sum((y - h) ** 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp , weight , bias , epochs , step , original_output):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        pred = nn(inp , weight , bias)\n",
    "        cost = cost_func(original_output , pred)\n",
    "        \n",
    "        up_weight = weight + step\n",
    "        down_weight = weight - step\n",
    "        up_bias = bias + step\n",
    "        down_bias = bias - step\n",
    "        \n",
    "        up_pred = nn(inp , up_weight , up_bias)\n",
    "        up_cost = cost_func(original_output , up_pred)\n",
    "        \n",
    "        down_pred = nn(inp , down_weight , down_bias)\n",
    "        down_cost = cost_func(original_output , down_pred)\n",
    "        \n",
    "        if((up_cost < cost) or (down_cost < cost)):\n",
    "            \n",
    "            if(up_cost < down_cost):\n",
    "                weight = up_weight\n",
    "                bias = up_bias\n",
    "                print('Loss --->', up_cost)\n",
    "            elif(down_cost < up_cost):\n",
    "                weight = down_weight\n",
    "                bias = down_bias\n",
    "                print('Loss --->' , down_cost)\n",
    "            else:\n",
    "                print('Loss --->', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ---> 0.15473307558098143\n",
      "Loss ---> 0.15293469170470134\n",
      "Loss ---> 0.15121408802188646\n",
      "Loss ---> 0.14956782106318\n",
      "Loss ---> 0.147992591758209\n",
      "Loss ---> 0.14648524075550534\n",
      "Loss ---> 0.14504274374126133\n",
      "Loss ---> 0.1436622067757407\n",
      "Loss ---> 0.14234086166409032\n",
      "Loss ---> 0.14107606137636608\n",
      "Loss ---> 0.13986527552977324\n",
      "Loss ---> 0.13870608594443554\n",
      "Loss ---> 0.13759618228243886\n",
      "Loss ---> 0.1365333577784565\n",
      "Loss ---> 0.13551550506892593\n",
      "Loss ---> 0.1345406121255325\n",
      "Loss ---> 0.1336067582976394\n",
      "Loss ---> 0.13271211046729472\n",
      "Loss ---> 0.13185491931952575\n",
      "Loss ---> 0.13103351572980781\n",
      "Loss ---> 0.1302463072698533\n",
      "Loss ---> 0.1294917748322051\n",
      "Loss ---> 0.12876846937353098\n",
      "Loss ---> 0.12807500877599903\n",
      "Loss ---> 0.12741007482565603\n",
      "Loss ---> 0.12677241030634093\n",
      "Loss ---> 0.1261608162073181\n",
      "Loss ---> 0.1255741490425269\n",
      "Loss ---> 0.12501131827909773\n",
      "Loss ---> 0.1244712838725768\n",
      "Loss ---> 0.12395305390613676\n",
      "Loss ---> 0.12345568233091375\n",
      "Loss ---> 0.1229782668045102\n",
      "Loss ---> 0.12251994662462239\n",
      "Loss ---> 0.12207990075470293\n",
      "Loss ---> 0.12165734593853544\n",
      "Loss ---> 0.12125153490058602\n",
      "Loss ---> 0.12086175462900334\n",
      "Loss ---> 0.12048732473815695\n",
      "Loss ---> 0.12012759590763353\n",
      "Loss ---> 0.11978194839465951\n",
      "Loss ---> 0.11944979061696305\n",
      "Loss ---> 0.11913055780315535\n",
      "Loss ---> 0.11882371070777095\n",
      "Loss ---> 0.11852873438818756\n",
      "Loss ---> 0.11824513704071075\n",
      "Loss ---> 0.11797244889319784\n",
      "Loss ---> 0.11771022115167354\n",
      "Loss ---> 0.11745802499847152\n",
      "Loss ---> 0.11721545063952417\n",
      "Loss ---> 0.11698210639850767\n",
      "Loss ---> 0.11675761785563085\n",
      "Loss ---> 0.11654162702894758\n",
      "Loss ---> 0.11633379159614904\n",
      "Loss ---> 0.11613378415488172\n",
      "Loss ---> 0.11594129151971355\n",
      "Loss ---> 0.1157560140539524\n",
      "Loss ---> 0.11557766503459854\n",
      "Loss ---> 0.11540597004878883\n",
      "Loss ---> 0.11524066642016362\n",
      "Loss ---> 0.11508150266365988\n",
      "Loss ---> 0.11492823796730006\n",
      "Loss ---> 0.11478064169961948\n",
      "Loss ---> 0.11463849294143127\n",
      "Loss ---> 0.11450158004069848\n",
      "Loss ---> 0.11436970018933426\n",
      "Loss ---> 0.11424265902081737\n",
      "Loss ---> 0.11412027022755694\n",
      "Loss ---> 0.11400235519699861\n",
      "Loss ---> 0.1138887426655138\n",
      "Loss ---> 0.1137792683891597\n",
      "Loss ---> 0.11367377483044669\n",
      "Loss ---> 0.11357211086029312\n",
      "Loss ---> 0.11347413147438667\n",
      "Loss ---> 0.11337969752321812\n",
      "Loss ---> 0.11328867545508244\n",
      "Loss ---> 0.11320093707138816\n",
      "Loss ---> 0.1131163592936419\n",
      "Loss ---> 0.11303482394151385\n",
      "Loss ---> 0.1129562175214183\n",
      "Loss ---> 0.11288043102507461\n",
      "Loss ---> 0.11280735973753872\n",
      "Loss ---> 0.11273690305422826\n",
      "Loss ---> 0.11266896430648303\n",
      "Loss ---> 0.11260345059523044\n",
      "Loss ---> 0.11254027263234831\n",
      "Loss ---> 0.11247934458933741\n",
      "Loss ---> 0.11242058395293693\n",
      "Loss ---> 0.11236391138733641\n",
      "Loss ---> 0.11230925060265633\n",
      "Loss ---> 0.11225652822938367\n",
      "Loss ---> 0.11220567369847018\n",
      "Loss ---> 0.11215661912681243\n",
      "Loss ---> 0.11210929920784918\n",
      "Loss ---> 0.1120636511070266\n",
      "Loss ---> 0.11201961436189181\n",
      "Loss ---> 0.11197713078659252\n",
      "Loss ---> 0.1119361443805661\n",
      "Loss ---> 0.11189660124121872\n",
      "Loss ---> 0.11185844948040291\n"
     ]
    }
   ],
   "source": [
    "weight = np.array([[0.1,0.2,0.3],[0.4,0.5,0.6]])\n",
    "bias = np.array([1,2])\n",
    "inp = np.array([1,2,3])\n",
    "original_out = [0.95 , 1.23]\n",
    "step = 0.005\n",
    "epochs = 100\n",
    "\n",
    "train(inp , weight , bias , epochs , step , original_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
